{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch CUDA framework\n",
    "* select gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selection GPU idx : 1\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "print(\"selection GPU idx : {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customDataLoader2 import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_image_loader(path):\n",
    "    try:\n",
    "        #print(path.split(\"/\")[-1])\n",
    "        img = cv.imread(path)\n",
    "    except:\n",
    "        return 0\n",
    "    img = cv.resize(img, (64, 64), interpolation = cv.INTER_AREA)\n",
    "    #img = np.expand_dims(img, axis=0)\n",
    "    #print(img.shape)\n",
    "    img = img.astype(np.float32)\n",
    "    cv.normalize(img, img, 0, 1, cv.NORM_MINMAX)\n",
    "    img = np.transpose(img, [2,0,1])\n",
    "    img = torch.tensor(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    # DataLoader( True, , , )  ==> get Training data set.\n",
    "    train_loader = torch.utils.data.DataLoader( DataLoader(True, default_image_loader), batch_size = 10, shuffle = True)\n",
    "    #train_loader = torch.utils.data.DataLoader( DataLoader(False, default_image_loader), batch_size = 5, shuffle = True)\n",
    "    '''\n",
    "    Python Generator\n",
    "    *  if you do run this line, you can get a data like the example\n",
    "    example:\n",
    "    gen = data_generator()\n",
    "    \n",
    "    print(next(gen)) # get data index number 1\n",
    "    print(next(gen)) # get data index number 2\n",
    "    \n",
    "    * as well as, you can uses with loop by enumerate\n",
    "    for i, data in enumerate(gen, 0):\n",
    "        print(data)\n",
    "    '''\n",
    "    \n",
    "    for data, label in train_loader:\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance of Neural Network Model class\n",
    "1) 뉴럴 네트워크 모델을 인스턴스화 한다.<br>\n",
    "2) ```model.cuda()```는 해당 뉴럴 모델을 GPU 연산으로 처리하고 싶을 때 사용한다.<br>\n",
    "3) Optimzer는 정답과 추측값의 오차를 줄이기 위하여 어떤 방법으로 오차를 줄여 나갈 지 계산하는 메서드<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(3, 3), bias=False)\n",
       "  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (norm0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_drop): Dropout2d(p=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=128, bias=False)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=False)\n",
       "  (fc3): Linear(in_features=64, out_features=20, bias=False)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instance model.\n",
    "model = Net()\n",
    "\n",
    "#-------------------------------------------------#\n",
    "#      Chenck points loads\n",
    "#      What is Check points? \n",
    "#      This is called a pre trained model\n",
    "#-------------------------------------------------#\n",
    "#PATH = \"./checkpoint.pt\"\n",
    "#checkpoint = torch.load(PATH, map_location=torch.device('cuda:0'))\n",
    "#checkpoint.pop('fc.weight', None) # Remove dict-key if current mode has not that.\n",
    "#checkpoint.pop('fc.bias', None) # Remove dict-key if current mode has not that.\n",
    "\n",
    "#new = model.state_dict() \n",
    "#new.update(checkpoint)\n",
    "#model.load_state_dict(new)\n",
    "#-------------------------------------------------#\n",
    "\n",
    "model.cuda() # GPU mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss fuction and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Sthochastic Gradient Discent, learning rate is 0.001, momentum = 0.9\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(epochs):\n",
    "    print(\"*** Trainning ***\")\n",
    "    is_cuda = False\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"*** CUDA Tranning ***\")\n",
    "        is_cuba = True\n",
    "\n",
    "    '''\n",
    "    OneHot encoding\n",
    "    '''\n",
    "    onehot = torch.zeros([21, 20])\n",
    "    for index in range(0,20):\n",
    "        onehot[index][index] = 1\n",
    "        #print(onehot[index])\n",
    "    print(onehot)\n",
    "    '''\n",
    "    Train the epochs\n",
    "    '''\n",
    "    for epoch in range(0,epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # data generator\n",
    "        gen = data_generator()\n",
    "        for i, data in enumerate(gen, 0):\n",
    "            images, labels = data\n",
    "            \n",
    "            # prediction\n",
    "            outputs = model(images.cuda())\n",
    "            \n",
    "            targets = onehot[labels]\n",
    "            #targets = targets.argmax(dim=1) \n",
    "            \n",
    "            loss = criterion(outputs, targets.cuda()) \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss rate\n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                print('[%d, %d] loss : %.3f' % (epoch + 1, i + 1, (running_loss / (epochs + 1)) * 100 ), \"%\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        #print(outputs)\n",
    "    print(\"Finish Trainning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Trainning ***\n",
      "*** CUDA Tranning ***\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:83: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1000] loss : 20.345 %\n",
      "[2, 1000] loss : 19.980 %\n",
      "[3, 1000] loss : 19.918 %\n",
      "[4, 1000] loss : 19.967 %\n",
      "[5, 1000] loss : 19.723 %\n",
      "[6, 1000] loss : 19.981 %\n",
      "[7, 1000] loss : 20.189 %\n",
      "[8, 1000] loss : 19.995 %\n",
      "[9, 1000] loss : 19.922 %\n",
      "[10, 1000] loss : 20.216 %\n",
      "[11, 1000] loss : 19.532 %\n",
      "[12, 1000] loss : 19.568 %\n",
      "[13, 1000] loss : 19.809 %\n",
      "[14, 1000] loss : 19.494 %\n",
      "[15, 1000] loss : 19.700 %\n",
      "[16, 1000] loss : 19.527 %\n",
      "[17, 1000] loss : 19.234 %\n",
      "[18, 1000] loss : 19.370 %\n",
      "[19, 1000] loss : 19.358 %\n",
      "[20, 1000] loss : 19.627 %\n",
      "[21, 1000] loss : 19.557 %\n",
      "[22, 1000] loss : 19.137 %\n",
      "[23, 1000] loss : 19.403 %\n",
      "[24, 1000] loss : 19.253 %\n",
      "[25, 1000] loss : 19.188 %\n",
      "[26, 1000] loss : 19.597 %\n",
      "[27, 1000] loss : 18.926 %\n",
      "[28, 1000] loss : 19.263 %\n",
      "[29, 1000] loss : 19.213 %\n",
      "[30, 1000] loss : 19.163 %\n",
      "[31, 1000] loss : 19.161 %\n",
      "[32, 1000] loss : 19.018 %\n",
      "[33, 1000] loss : 18.803 %\n",
      "[34, 1000] loss : 19.338 %\n",
      "[35, 1000] loss : 18.972 %\n",
      "[36, 1000] loss : 18.740 %\n",
      "[37, 1000] loss : 19.092 %\n",
      "[38, 1000] loss : 18.962 %\n",
      "[39, 1000] loss : 19.005 %\n",
      "[40, 1000] loss : 19.130 %\n",
      "[41, 1000] loss : 18.459 %\n",
      "[42, 1000] loss : 19.008 %\n",
      "[43, 1000] loss : 19.129 %\n",
      "[44, 1000] loss : 18.551 %\n",
      "[45, 1000] loss : 18.665 %\n",
      "[46, 1000] loss : 18.725 %\n",
      "[47, 1000] loss : 18.768 %\n",
      "[48, 1000] loss : 18.488 %\n",
      "[49, 1000] loss : 18.728 %\n",
      "[50, 1000] loss : 18.754 %\n",
      "[51, 1000] loss : 18.495 %\n",
      "[52, 1000] loss : 18.801 %\n",
      "[53, 1000] loss : 18.777 %\n",
      "[54, 1000] loss : 18.487 %\n",
      "[55, 1000] loss : 18.703 %\n",
      "[56, 1000] loss : 18.742 %\n",
      "[57, 1000] loss : 18.329 %\n",
      "[58, 1000] loss : 18.250 %\n",
      "[59, 1000] loss : 18.419 %\n",
      "[60, 1000] loss : 18.730 %\n",
      "[61, 1000] loss : 18.659 %\n",
      "[62, 1000] loss : 18.456 %\n",
      "[63, 1000] loss : 18.165 %\n",
      "[64, 1000] loss : 18.336 %\n",
      "[65, 1000] loss : 18.233 %\n",
      "[66, 1000] loss : 18.377 %\n",
      "[67, 1000] loss : 17.959 %\n",
      "[68, 1000] loss : 18.173 %\n",
      "[69, 1000] loss : 18.339 %\n",
      "[70, 1000] loss : 18.083 %\n",
      "[71, 1000] loss : 17.923 %\n",
      "[72, 1000] loss : 18.192 %\n",
      "[73, 1000] loss : 18.388 %\n",
      "[74, 1000] loss : 17.860 %\n",
      "[75, 1000] loss : 17.918 %\n",
      "[76, 1000] loss : 18.160 %\n",
      "[77, 1000] loss : 17.602 %\n",
      "[78, 1000] loss : 18.039 %\n",
      "[79, 1000] loss : 18.081 %\n",
      "[80, 1000] loss : 18.124 %\n",
      "[81, 1000] loss : 18.160 %\n",
      "[82, 1000] loss : 17.960 %\n",
      "[83, 1000] loss : 18.029 %\n",
      "[84, 1000] loss : 17.995 %\n",
      "[85, 1000] loss : 17.660 %\n",
      "[86, 1000] loss : 17.687 %\n",
      "[87, 1000] loss : 18.121 %\n",
      "[88, 1000] loss : 17.860 %\n",
      "[89, 1000] loss : 17.928 %\n",
      "[90, 1000] loss : 17.385 %\n",
      "[91, 1000] loss : 17.567 %\n",
      "[92, 1000] loss : 17.617 %\n",
      "[93, 1000] loss : 17.918 %\n",
      "[94, 1000] loss : 17.526 %\n",
      "[95, 1000] loss : 17.734 %\n",
      "[96, 1000] loss : 17.746 %\n",
      "[97, 1000] loss : 17.618 %\n",
      "[98, 1000] loss : 17.469 %\n",
      "[99, 1000] loss : 17.628 %\n",
      "[100, 1000] loss : 17.294 %\n",
      "Finish Trainning\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testLoader():\n",
    "    test_loader = torch.utils.data.DataLoader( DataLoader(False, default_image_loader), batch_size = 10, shuffle = True)\n",
    "    for data, label in test_loader:\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def test():\n",
    "    to_img = ToPILImage()\n",
    "    total = 0\n",
    "    list = {}\n",
    "    model.cuda()\n",
    "    gen = testLoader()\n",
    "    sum = 0\n",
    "    num = 0\n",
    "    for i, data in enumerate(gen, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs.cuda())\n",
    "        #print(labels, torch.argmax(outputs,dim=1))\n",
    "        '''\n",
    "        for input in inputs:\n",
    "            img_frame = torch.Tensor(input).nomal_()\n",
    "            to_img(img_frame)\n",
    "        '''\n",
    "        #print(labels, \"\\n\", torch.argmax(outputs,dim=1))\n",
    "        for j in range(len(labels)):\n",
    "            num = num + 1\n",
    "            if labels[j].item() == torch.argmax(outputs, dim=1)[j].item():\n",
    "                #print(labels[j].item(), torch.argmax(outputs, dim=1)[j].item())\n",
    "                sum = sum + 1\n",
    "    \n",
    "    print(\"Validation dataset : {}\".format(num))\n",
    "    print(\"Total Accuracy \\n\", (sum / 2013) * 100, \"%\" )\n",
    "    #return labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:83: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.54992548435171 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 63.785394932935915 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.99701937406855 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.940884252359666 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.14605067064084 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.1525086934923 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.04669647292598 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.24540486835569 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 63.189269746646794 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.49379036264282 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.74863387978142 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.74863387978142 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.59960258320915 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.54992548435171 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.798310978638845 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.847988077496275 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.295081967213115 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.4441132637854 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.30153999006458 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.25186289120715 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 60.457029309488334 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.847988077496275 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.1525086934923 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.50024838549428 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.4441132637854 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.0963735717834 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.14605067064084 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.791852955787384 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.4441132637854 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.1525086934923 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.74863387978142 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.791852955787384 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.49379036264282 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 63.139592647789364 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.50024838549428 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 60.35767511177347 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 60.90412319920516 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.04669647292598 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 60.50670640834576 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.0963735717834 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.99701937406855 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.940884252359666 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.24540486835569 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.0963735717834 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.45057128663686 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.798310978638845 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 63.437655240933935 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 62.692498758072524 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.847988077496275 %\n",
      "Validation dataset : 2013\n",
      "Total Accuracy \n",
      " 61.64927968206657 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 50): test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
